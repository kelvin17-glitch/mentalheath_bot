# -*- coding: utf-8 -*-
"""mentalmate_workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oEE9DDmfmrdVmBiq8SgMIW4CpxOcqjc5

### INTENT AND EMOTION DETECTION USING DistilBERT
"""

# Import required libraries
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
import torch
import torch.nn.functional as F

# Define intent keywords
intent_keywords = {
    "venting": ["frustrated", "angry", "fed up", "tired", "exhausted"],
    "seeking_advice": ["what should I do", "any suggestions", "advice", "recommend"],
    "gratitude": ["thank you", "appreciate", "grateful", "thanks a lot"],
    "general_chat": ["how are you", "what's up", "hello", "hi"],
}
# Define personas
personas = {
    "Zen_guide": "You are a Zen Guide. You offer calm and mindful responses. Your objective is to offer advice on how to achieve inner peace through meditation and reflection.",
    "Cheerful_companion": " You are an uplifting friend who brings positivity and encouragement to brighten one's day.",
    "Wise_owl": "You are a thoughtful mentor offering deep insights and practical wisdom for life's challenges.",
    "Gentle_soul": "You are a tender, empathetic listener who provides comfort during difficult times.",
    "Resilient_warrior": "You are a strong, determined guide who helps one build courage and overcome obstacles.",
    "Balanced_soul": "You are a harmonious presence that helps one find equilibrium in all aspects of life."
}

# Receive user input
user_input = input("How are you feeling today? ")
persona = input("Choose a persona (Zen_guide, Cheerful_companion, Wise_owl, Gentle_soul, Resilient_warrior, Balanced_soul): ")

# Intent detection
def detect_intent(text):
  text = text.lower()
  for intent, keywords in intent_keywords.items():
    if any(keyword in text for keyword in keywords):
      return intent
  return "general_chat"
intent = detect_intent(user_input)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('nateraw/bert-base-uncased-emotion')
model = AutoModelForSequenceClassification.from_pretrained('nateraw/bert-base-uncased-emotion')

# Preprocess input
inputs = tokenizer(user_input, return_tensors="pt")

# Run through the model
with torch.no_grad():
  outputs = model(**inputs)

# Get emotion probabilities
probs = F.softmax(outputs.logits, dim=1)
predicted_emotion = torch.argmax(probs, dim=1).item()
emotion_label = model.config.id2label[predicted_emotion]

"""TEXT GENERATION"""

# Load the generator
tokenizer = AutoTokenizer.from_pretrained("openchat/openchat-3.5-0106")
model = AutoModelForCausalLM.from_pretrained("openchat/openchat-3.5-0106")

# Simulated results from previous modules
emotion = emotion_label

# Build custom prompt
prompt = f"{personas[persona]} I am feeling {emotion}. {user_input}"

# Tokenize and generate
inputs = tokenizer(prompt, return_tensors="pt")
output = model.generate(
    **inputs,
    max_new_tokens=150,
    eos_token_id=tokenizer.eos_token_id,
    do_sample=True,
    top_k=50,
    top_p=0.95,
)

# Decode the response
response = tokenizer.decode(output[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
# Catch incomplete output
if not response.strip().endswith(('.', '!', '?')):
    # Assume it's incomplete
    continue_prompt = output + "..."
    next_output = model.generate(continue_prompt)
    output = output + next_output
    response = tokenizer.decode(output[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)

# Extract only bot's response
print(response.strip())